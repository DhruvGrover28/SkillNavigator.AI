#!/usr/bin/env python3
"""
Test the improved scraper to fetch 20-30 real job postings
"""

import asyncio
import logging
import sys
import os

# Add backend to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from agents.enhanced_scraper_agent import EnhancedScraperAgent

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def test_improved_scraper():
    """Test improved scraper with better keyword matching"""
    
    print("ğŸš€ Testing Improved Scraper for 20-30 Real Jobs...")
    
    # Create enhanced scraper
    scraper = EnhancedScraperAgent()
    
    try:
        # Initialize scraper
        await scraper.initialize()
        print("âœ… Enhanced scraper initialized")
        
        all_jobs = []
        
        # Test with broader search terms for variety
        search_queries = [
            {'keywords': 'software', 'max_results': 12},    # Should match many jobs
            {'keywords': 'developer', 'max_results': 10},   # Should match many jobs
            {'keywords': 'engineer', 'max_results': 8},     # Should match many jobs
            {'keywords': 'technical', 'max_results': 5},    # Should match some jobs
        ]
        
        for i, query in enumerate(search_queries, 1):
            print(f"\nğŸ” Search {i}: '{query['keywords']}' (targeting {query['max_results']} jobs)")
            
            jobs = await scraper.scrape_jobs(query)
            print(f"ğŸ“Š Found {len(jobs)} jobs for '{query['keywords']}'")
            
            # Show first 3 jobs from this search
            for j, job in enumerate(jobs[:3]):
                print(f"  {j+1}. ğŸ¢ {job.title} at {job.company}")
                print(f"     ğŸ“ {job.location}")
                print(f"     ğŸ”— {job.url}")
            
            all_jobs.extend(jobs)
            
            # Small delay between searches
            await asyncio.sleep(1)
        
        # Remove any duplicates by URL
        seen_urls = set()
        unique_jobs = []
        for job in all_jobs:
            if job.url not in seen_urls:
                seen_urls.add(job.url)
                unique_jobs.append(job)
        
        print(f"\nğŸ“ˆ TOTAL UNIQUE JOBS COLLECTED: {len(unique_jobs)}")
        print("=" * 70)
        
        # Show all unique jobs with full details
        print("ğŸ“‹ Complete Unique Job Listings:")
        for i, job in enumerate(unique_jobs, 1):
            print(f"\n{i:2d}. ğŸ¢ {job.title}")
            print(f"     Company: {job.company}")
            print(f"     Location: {job.location}")
            print(f"     Full URL: {job.url}")
            if job.salary:
                print(f"     Salary: {job.salary}")
            if job.skills and len(job.skills) > 0:
                print(f"     Skills: {', '.join(job.skills[:4])}")
            
        # Verify URLs are full RemoteOK URLs
        real_urls = []
        for job in unique_jobs:
            url = job.url
            if 'remoteok.com/remote-jobs/remote-' in url and len(url) > 60:
                real_urls.append(url)
        
        print(f"\nğŸ”— URL Analysis:")
        print(f"âœ… Full RemoteOK URLs: {len(real_urls)}")
        print(f"ğŸ“Š Total unique jobs: {len(unique_jobs)}")
        
        if len(real_urls) >= 20:
            print(f"ğŸ‰ SUCCESS: Got {len(real_urls)} real job URLs (target: 20-30)")
        else:
            print(f"âš ï¸ Need more jobs. Got {len(real_urls)} real URLs (target: 20-30)")
        
        return unique_jobs
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return []
    finally:
        try:
            await scraper.cleanup()
            print("\nğŸ§¹ Cleanup completed")
        except:
            pass

if __name__ == "__main__":
    jobs = asyncio.run(test_improved_scraper())
    if len(jobs) >= 20:
        print(f"\nğŸ‰ Successfully fetched {len(jobs)} real job postings!")
    else:
        print(f"\nâš ï¸ Only got {len(jobs)} jobs - may need more improvements")